\section{Modified Mini Batch $k$-Means Algorithm}
\label{sec:TheoryKMeans}
The $k$-Means algorithm takes $k$ random points in space representing the center of $k$ classes. These class centers are then refined in two steps: First, each data point in a set of training data is classified to belong to one of the classes by selecting the closest class center. Second, the class centers are moved towards the geometric center of all the data points classified as belonging to the class. These steps are repeated until the class centers stop moving.\\
However, this approach is impractical for large data sets, since every iteration needs to calculate distances from every data point to every class center. A modification of this algorithm called Mini-batch $k$-Means \cite{bib:Elkan2003} uses only a random subset of all data points for each iteration. This results in slightly less accurate class centers, but much greater performance.\\
In our case though, there is no euclidean feature space in which we could position class centers. The only measure available is the DTW-distance between the different samples. Thus, we modified the second step of the Mini-batch $k$-Means algorithm to choose the center-most sample instead of the arithmetic center. The center-most sample is defined as the one sample in the class that has the least distance to all other samples in the class.\\
Running this algorithm on our data set results in $k$ class centers. New samples can then be classified to belong to one of these centers by selecting the closest one.\\
Calculating class centers requires the calculation of many distances between many samples. In order to speed up this process, we pre-calculated all the sample distances as described in chapter \ref{sec:Distance}.
