\section{$k$-Nearest-Neighbors Algorithm}
\label{sec:TheoryKnn}
The $k$-Nearest-Neighbors ($k$-NN) algorithm performs a distribution free classification task. It uses a lazy learning approach which means that all the training data is saved. The Algorithm searches that data base for the $k$ closest data points to the point to classify. The distance measurement depends on the application. The class which is most often found in those $k$ nearest neighbors is the one the sample is classified as \cite[p.~338~f.]{bib:Alzate2007}.\\
The space of the data points is the PCA-reduced feature space described in section~\ref{sec:Features}. The distance used in our work is the DTW-distance described in section~\ref{sec:Distance}. The feature space version of a test sample is calculated and compared to all other feature vectors in the database from section~\ref{sec:Features}. This comparison table is then sorted by distance using the sorting algorithm built into Python. As output of the $k$-NN classification the ratio of found neighbors to the total number of neighbors is given for the $k$ nearest neighbors. This can be interpreted as the probability of a point belonging to a class.
